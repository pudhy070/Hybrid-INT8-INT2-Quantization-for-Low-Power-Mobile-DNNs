# Adaptive Multi-Precision Quantization for Ultra-Low-Power Mobile Environments: A Sensitivity-Aware Optimization Framework for Hybrid INT8‚ÄìINT2 Deep Neural Networks

Author: Sia

## Abstract
As mobile and IoT edge devices increasingly depend on on-device artificial intelligence, the demand for high-efficiency deep learning inference under stringent power and memory constraints has intensified. Conventional uniform-precision quantization methods‚Äîprimarily INT8‚Äîhave delivered improvements in latency and model size but remain insufficient for next-generation ultra-low-power platforms such as wearables, smart sensors, and battery-limited robotics. This thesis presents an Adaptive Multi-Precision Quantization (AMPQ) framework that assigns heterogeneous bit-widths (INT8, INT4, INT2) across neural network layers. The proposed Sensitivity-Aware Layer Selector (SALS) algorithm quantifies the quantization vulnerability of each layer and dynamically determines an optimal bit allocation strategy that balances efficiency with accuracy preservation. Experiments on a commercial mobile application processor demonstrate that AMPQ achieves up to 6.3√ó compression over FP32 and 38% average latency reduction relative to uniform INT8, while maintaining Top-1 accuracy within 1% of the baseline. These results confirm the feasibility of hybrid low-bit quantization as a practical solution for energy-efficient inference in resource-constrained edge environments.
 
## Chapter 1. Introduction
### 1.1 Research Background
On-device AI has emerged as a vital component of modern mobile computing, driven by the need for privacy preservation, reduced reliance on network connectivity, and real-time responsiveness. However, the deployment of deep neural networks (DNNs) on mobile processors introduces significant challenges. State-of-the-art DNNs contain millions of parameters and require billions of floating-point operations, leading to substantial memory traffic and energy consumption.

In battery-powered IoT, wearable, and embedded systems, device lifetime is directly linked to computational efficiency. Even well-optimized FP32 or FP16 models impose power burdens unsuitable for continuous or real-time operation. As a result, model compression techniques that reduce memory footprint and arithmetic complexity are now indispensable for practical on-device inference.

### 1.2 Problem Definition and Limitations of Prior Work

Quantization is one of the most effective compression approaches and typically involves converting floating-point parameters into fixed-point integers (commonly INT8). While INT8 quantization has demonstrated strong performance across various hardware platforms, several limitations persist:

- Uniform Bit-widths Are Not Optimal:
Applying the same precision across all layers disregards the distinct sensitivity and structural characteristics of individual layers.

- Ultra-Low-Bit Quantization Faces Accuracy Collapse:
Quantization below 4 bits (e.g., INT2) yields drastic compression but often introduces severe accuracy degradation when applied uniformly.

- Existing Mixed-Precision Methods Are Expensive:
Reinforcement Learning (RL) based quantization search methods, such as HAQ, incur substantial computational overhead and are impractical for rapid deployment.

These limitations highlight the need for a practical and computationally lightweight strategy for determining per-layer precision that preserves accuracy while achieving ultra-low energy consumption.
### 1.3 Research Objective

The objective of this research is to design a deterministic, sensitivity-driven, multi-precision quantization framework capable of:

- Assigning optimal bit-widths (INT8/INT4/INT2) on a per-layer basis
- Maintaining model accuracy close to FP32
- Reducing model size and latency for mobile devices
- Operating efficiently without the search cost of RL-based methods

### 1.4 Major Contributions

This thesis makes the following contributions:

1. Proposal of AMPQ (Adaptive Multi-Precision Quantization):
A hybrid precision assignment method that maximizes compression without uniform accuracy sacrifice.

2. Introduction of SALS (Sensitivity-Aware Layer Selector):
A novel algorithm that evaluates quantization vulnerability and performs deterministic precision allocation.

3. Comprehensive experimental evaluation on mobile hardware:
Using Qualcomm Snapdragon DSP accelerators with TFLite-based deployment.

4. Demonstration of practical edge-AI viability:
Showing that INT2-integrated models can operate within a 1% accuracy drop while significantly reducing power and latency.
## Chapter 2. Related Work
### 2.1 Model Quantization Techniques

Quantization maps high-precision floating-point weights into low-bit integer formats to reduce memory and computational overhead. Prior studies have explored:

- Fixed-point quantization (INT8)
- Symmetric vs. asymmetric scaling
- Post-training quantization (PTQ) and quantization-aware training (QAT)
- Extreme low-bit quantization (binary/ternary)

While these techniques provide varying levels of compression, uniform precision assignments often fail to respect the structural diversity of model layers.

### 2.2 Mixed-Precision Quantization

Mixed-precision approaches allow per-layer bit assignment. Notable methods include:
- HAQ: RL-based hardware-aware bit search
- ProxylessNAS-based quantization search
- Gradient-based precision optimization

Although effective, these methods tend to incur large computational costs, making them difficult to deploy in real-world engineering workflows.
### 2.3 Sensitivity-Based Optimization

Sensitivity analysis evaluates how perturbations in weights influence overall loss. Prior work has investigated:

- Hessian-based sensitivity metrics
- Disturbance-guided pruning
- Curvature-aware quantization

However, such methods have not been fully exploited for ultra-low-bit quantization in mobile environments.
## Chapter 3. Proposed Method
### 3.1 Multi-Precision Quantization Framework
Let a neural network model \( M \) consist of layers \( L_1, L_2, \dots, L_n \).  
Each layer is assigned a bit-width \( b_i \in B = \{8, 4, 2\} \).  

The precision assignment policy is governed by:

- Structural redundancy
- Distribution flatness
- Parameter count
- Sensitivity score
Precision Assignment Principles
| Bit-width | Application Target                                   | Rationale           |
| --------- | ---------------------------------------------------- | ------------------- |
|   INT8    | High-sensitivity layers, depthwise convs, I/O layers | Accuracy retention  |
|   INT4    | Moderately important standard conv layers            | Balanced trade-off  |
|   INT2    | High-redundancy layers (e.g., PW conv)               | Maximum compression |


### 3.2 Sensitivity-Aware Layer Selector (SALS)

To determine the optimal bit-width for each layer, SALS computes a sensitivity metric:
          Œ©i=Ex‚àºD [‚à•L(W)‚àíL(W+ŒîWi)‚à•2]

Where:
- ùêø: loss function
- ŒîWi: quantization-induced distortion

Algorithm Overview

1. Compute Œ©i‚Äã for each layer.
2. Rank layers from high to low sensitivity.
3. Assign INT8 to high-sensitivity layers.
4. Assign INT4 and INT2 progressively to less sensitive layers.
5. Stop when accuracy constraint is met.

This deterministic method avoids the costly search loops of RL-based approaches.
 
## Chapter 4. Experimental Results
### 4.1 Experimental Environment

- Device: Qualcomm Snapdragon-based Android tablet
- Framework: TensorFlow Lite (custom kernels for INT2)
- Benchmarks: MobileNetV3, ResNet-50
- Dataset: ImageNet validation set

### 4.2 Quantitative Results
MobileNetV3 Evaluation
| Model         | Method                    | Size (MB) | Compression Ratio | Latency (ms) | Top-1 Accuracy (%) |
| ------------- | ------------------------- | --------- | ----------------- | ------------ | ------------------ |
| FP32 Baseline | FP32                      | 22.0      | 1.0√ó              | 45.2         | 75.2               |
| Uniform Quant | INT8                      | 5.8       | 3.8√ó              | 28.5         | 74.8               |
|   Proposed    |   AMPQ (INT8+INT4+INT2)   | 3.5       | 6.3               | 17.6         | 74.5               |
Key Observations

1. Compression Efficiency:
AMPQ achieves 6.3√ó compression‚Äîsignificantly outperforming uniform INT8.
2. Inference Acceleration:
Reduced memory bandwidth pressure yields 38% faster inference than INT8.
3. Accuracy Stability:
Despite INT2 integration, the accuracy drop is limited to 0.3% vs. INT8, confirming robustness.
## Chapter 5. Discussion
### 5.1 Implications for Edge AI

The results indicate that:
- Ultra-low-bit quantization is feasible when sensitivity-aware strategies are applied.
- Model compression can directly translate to prolonged battery lifetime.
- AMPQ is compatible with existing mobile inference engines (e.g., TFLite DSP backends).

### 5.2 Limitations

- SALS currently requires partial dataset sampling for sensitivity evaluation.
- Custom INT2 operators must be supported by the hardware backend.

### 5.3 Future Research Directions

1. INT2-optimized accelerator design for commercial NPUs
2. AutoML-driven adaptive bit-width search
3. Integration with pruning and weight sharing
4. End-to-end compiler support for hybrid precision models
## Chapter 6. Conclusion

This thesis introduced Adaptive Multi-Precision Quantization (AMPQ), a hybrid INT8‚ÄìINT2 quantization framework that maximizes efficiency while preserving accuracy. The Sensitivity-Aware Layer Selector (SALS) algorithm provides a deterministic, practical approach for identifying optimal per-layer precision. Extensive experiments confirmed substantial gains in model size, latency, and energy efficiency, making AMPQ a promising solution for next-generation mobile and IoT edge computing.

The findings demonstrate that ultra-low precision (INT2) can be safely integrated into production-grade models when guided by structural redundancy and sensitivity metrics. The proposed framework contributes to the advancement of sustainable, battery-efficient on-device AI technology and opens avenues for future research in hardware-aware quantization.
